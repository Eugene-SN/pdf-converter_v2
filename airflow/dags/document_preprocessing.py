#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
‚úÖ –ü–ï–†–ï–†–ê–ë–û–¢–ê–ù–ù–´–ô DAG: Document Preprocessing - –ï–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
–í–ï–†–°–ò–Ø 3.0 - Production-ready —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö PDF

–ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø:
- ‚úÖ Airflow —Ç–æ–ª—å–∫–æ –∫–∞–∫ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä
- ‚úÖ –í—Å—è –ª–æ–≥–∏–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ —ç—Ç–æ–º DAG
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- ‚úÖ –ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Docling –±–µ–∑ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤
- ‚úÖ –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –Ω–æ –º–æ—â–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException
import os
import json
import logging
import time
from typing import Dict, Any, Optional
from pathlib import Path

# –ü—Ä—è–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–∏—Å document-processor)
import requests  # ‚Üê –≤—ã–Ω–æ—Å–∏–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –∏–∑ Airflow –≤ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å [–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è]
DOCUMENT_PROCESSOR_URL = os.getenv('DOCUMENT_PROCESSOR_URL', 'http://document-processor:8001')

# –£—Ç–∏–ª–∏—Ç—ã
from shared_utils import (
    SharedUtils, NotificationUtils, ConfigUtils, 
    MetricsUtils, ErrorHandlingUtils
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG
DEFAULT_ARGS = {
    'owner': 'pdf-converter',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=3),
}

dag = DAG(
    'document_preprocessing',  # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–º—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä—É
    default_args=DEFAULT_ARGS,
    description='DAG 1: –ï–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ Markdown –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤',
    schedule_interval=None,
    max_active_runs=3,
    catchup=False,
    tags=['pdf-converter', 'dag1', 'chinese-docs', 'production']
)

# ================================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø –ö–ò–¢–ê–ô–°–ö–ò–• –î–û–ö–£–ú–ï–ù–¢–û–í
# ================================================================================

# –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
CHINESE_DOC_CONFIG = {
    # OCR –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    'ocr_languages': 'chi_sim,chi_tra,eng',  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –∫–∏—Ç–∞–π—Å–∫–∏–π + –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
    'ocr_confidence_threshold': 0.75,  # –ü–æ–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    'chinese_header_patterns': [
        r'^[Á¨¨Á´†ËäÇ]\s*[‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅ\d]+\s*[Á´†ËäÇ]',  # Á¨¨XÁ´†, Á¨¨XËäÇ
        r'^[‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅ]+[„ÄÅÔºé]',  # –ö–∏—Ç–∞–π—Å–∫–∏–µ —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ
        r'^\d+[„ÄÅÔºé]\s*[\u4e00-\u9fff]',  # –ê—Ä–∞–±—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã + –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
    ],
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è (–ù–ï –ü–ï–†–ï–í–û–î–ò–¢–¨)
    'tech_terms': {
        'ÈóÆÂ§©': 'WenTian',
        'ËÅîÊÉ≥ÈóÆÂ§©': 'Lenovo WenTian', 
        'Â§©Êìé': 'ThinkSystem',
        'Ëá≥Âº∫': 'Xeon',
        'ÂèØÊâ©Â±ïÂ§ÑÁêÜÂô®': 'Scalable Processors',
        'Ëã±ÁâπÂ∞î': 'Intel',
        'Â§ÑÁêÜÂô®': 'Processor',
        'ÂÜÖÊ†∏': 'Core',
        'Á∫øÁ®ã': 'Thread',
        'ÂÜÖÂ≠ò': 'Memory',
        'Â≠òÂÇ®': 'Storage',
        '‰ª•Â§™ÁΩë': 'Ethernet',
        'Êú∫Êû∂': 'Rack',
        'ÊèíÊßΩ': 'Slot',
        'ÁîµÊ∫ê': 'Power Supply'
    },
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö PDF
    'quality_settings': {
        'dpi': 300,  # –í—ã—Å–æ–∫–æ–µ DPI –¥–ª—è —á–µ—Ç–∫–∏—Ö –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        'enable_table_detection': True,
        'preserve_chinese_formatting': True,
        'enhance_chinese_text': True
    }
}

# ================================================================================
# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò
# ================================================================================

def validate_input_file(**context) -> Dict[str, Any]:
    """‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    start_time = time.time()
    
    try:
        dag_run_conf = context['dag_run'].conf or {}
        logger.info(f"üìã –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {json.dumps(dag_run_conf, indent=2, ensure_ascii=False)}")
        
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        required_params = ['input_file', 'filename', 'timestamp']
        missing_params = [param for param in required_params if not dag_run_conf.get(param)]
        
        if missing_params:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_params}")

        master_run_id = dag_run_conf.get('master_run_id') or context['dag_run'].run_id
        if 'master_run_id' not in dag_run_conf:
            logger.info(f"üÜî master_run_id –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –≤–æ –≤—Ö–æ–¥–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                        f"–ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {master_run_id}")
        dag_run_conf['master_run_id'] = master_run_id  # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —Å—Ç–∞–¥–∏–π            
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        input_file = dag_run_conf['input_file']
        if not SharedUtils.validate_input_file(input_file):
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–∞–π–ª: {input_file}")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        file_info = analyze_chinese_document(input_file)
        
        # –û–±–æ–≥–∞—â–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        enriched_config = {
            **dag_run_conf,
            **file_info,
            'chinese_doc_analysis': file_info,
            'processing_mode': 'chinese_optimized',
            'validation_timestamp': datetime.now().isoformat()
        }
        
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='validate_input_file',
            processing_time=time.time() - start_time,
            success=True
        )
        
        logger.info(f"‚úÖ –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω: {dag_run_conf['filename']}")
        return enriched_config
        
    except Exception as e:
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='validate_input_file', 
            processing_time=time.time() - start_time,
            success=False
        )
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        raise

def analyze_chinese_document(file_path: str) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    try:
        file_size = os.path.getsize(file_path)
        file_hash = SharedUtils.calculate_file_hash(file_path)
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ PDF —á–∏—Ç–∞–µ—Ç—Å—è)
        has_chinese_text = False
        estimated_pages = 0
        
        try:
            # –ü—Ä–æ–±—É–µ–º –±—ã—Å—Ç—Ä–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
            import fitz  # PyMuPDF –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            doc = fitz.open(file_path)
            estimated_pages = doc.page_count
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
            for page_num in range(min(3, doc.page_count)):
                page = doc[page_num]
                text = page.get_text()[:1000]  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
                chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
                if chinese_chars > 10:  # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –±–æ–ª–µ–µ 10 –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    has_chinese_text = True
                    break
            
            doc.close()
            
        except Exception:
            # Fallback –∞–Ω–∞–ª–∏–∑ –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª–∞
            estimated_pages = max(1, file_size // 102400)  # ~100KB –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        return {
            'file_hash': file_hash,
            'file_size_bytes': file_size,
            'file_size_mb': file_size / (1024 * 1024),
            'estimated_pages': estimated_pages,
            'has_chinese_text': has_chinese_text,
            'recommended_ocr': not has_chinese_text,  # OCR –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è
            'processing_complexity': 'high' if file_size > 50*1024*1024 else 'medium'
        }
        
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}")
        return {
            'file_hash': 'unknown',
            'file_size_bytes': 0,
            'file_size_mb': 0.0,
            'estimated_pages': 1,
            'has_chinese_text': True,  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∫–∏—Ç–∞–π—Å–∫–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            'recommended_ocr': True,
            'processing_complexity': 'medium'
        }

def process_document_with_docling(**context) -> Dict[str, Any]:
    """‚úÖ –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–∏—Å document-processor
    (Docling –∏ OCR –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –¢–û–õ–¨–ö–û –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Å–µ—Ä–≤–∏—Å–µ, Airflow –∑–¥–µ—Å—å ‚Äî –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä)
    """
    start_time = time.time()
    config = context['task_instance'].xcom_pull(task_ids='validate_input_file')
    try:
        input_file = config['input_file']
        timestamp = config['timestamp']
        filename = config['filename']

        logger.info(f"üîÑ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º PDF –≤ document-processor: {DOCUMENT_PROCESSOR_URL}/process")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞ (—Ç–æ, —á—Ç–æ —Ä–∞–Ω–µ–µ –∑–∞–¥–∞–≤–∞–ª–æ—Å—å –≤ DAG)
        options = {
            "extract_tables": bool(config.get("extract_tables", True)),
            "extract_images": bool(config.get("extract_images", True)),
            "extract_formulas": bool(config.get("extract_formulas", True)),
            "use_ocr": bool(config.get("enable_ocr", config.get("chinese_doc_analysis", {}).get("recommended_ocr", False))),
            "ocr_languages": config.get("ocr_languages", "eng,chi_sim"),
            "high_quality_ocr": bool(config.get("high_quality_ocr", True)),
            "preserve_layout": bool(config.get("preserve_structure", True)),
            "enable_chunking": False
        }

        # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥: TEMP_DIR ‚Üí processing_paths.temp_dir ‚Üí $AIRFLOW_HOME/temp
        airflow_home_temp = os.path.join(os.getenv('AIRFLOW_HOME', '/opt/airflow'), 'temp')  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ–±—Ä–∞–∑–æ–≤ Airflow
        candidates = [
            airflow_home_temp,
            os.getenv('TEMP_DIR'),
            (ConfigUtils.get_processing_paths().get('temp_dir') if 'ConfigUtils' in globals() else None)
        ]
        temp_root = None
        for cand in candidates:
            if not cand:
                continue
            try:
                os.makedirs(cand, exist_ok=True)
                temp_root = cand
                break
            except PermissionError:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∫–∞—Ç–∞–ª–æ–≥ {cand}, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π")
        if not temp_root:
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º $AIRFLOW_HOME/temp
            temp_root = airflow_home_temp
            os.makedirs(temp_root, exist_ok=True)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –∏ –æ–ø—Ü–∏–∏ –≤ —Å–µ—Ä–≤–∏—Å
        with open(input_file, 'rb') as f:
            files = {'file': (os.path.basename(input_file), f, 'application/pdf')}
            data = {'options': json.dumps(options, ensure_ascii=False)}
            resp = requests.post(f"{DOCUMENT_PROCESSOR_URL}/process", files=files, data=data, timeout=60*30)

        if resp.status_code != 200:
            err = f"document-processor –≤–µ—Ä–Ω—É–ª {resp.status_code}: {resp.text}"
            logger.error(f"‚ùå {err}")
            return {"success": False, "error": err, "original_config": config}

        resp_json = resp.json()
        if not resp_json.get("success", False):
            err = f"document-processor —Å–æ–æ–±—â–∏–ª –æ–± –æ—à–∏–±–∫–µ: {resp_json.get('message') or resp_json.get('error')}"
            logger.error(f"‚ùå {err}")
            return {"success": False, "error": err, "original_config": config}

        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª, –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –∏–∑ –ø–æ–ª—è 'intermediate_file'
        intermediate_file = resp_json.get("intermediate_file")
        if not intermediate_file:
            # fallback: –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å –∏–∑ output_files –ø–µ—Ä–≤—ã–π JSON
            for p in resp_json.get("output_files", []):
                if str(p).endswith("_intermediate.json") or str(p).endswith(".json"):
                    intermediate_file = p
                    break

        if not intermediate_file or not os.path.exists(intermediate_file):
            # –ï—Å–ª–∏ —Å–µ—Ä–≤–∏—Å –≤–µ—Ä–Ω—É–ª –ø—É—Ç—å –≤ —Å–≤–æ—ë–º work_dir, –Ω–æ –æ–Ω –Ω–µ —à–∞—Ä–∏—Ç—Å—è —Å Airflow,
            # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–µ–∑–Ω—ã–π –º–∏–Ω–∏–º—É–º —Ä—è–¥–æ–º (–∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω—ã–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç)
            local_intermediate = os.path.join(temp_root, f"preprocessing_{timestamp}.json")
            safe_payload = {
                "title": resp_json.get("document_id", filename.replace('.pdf', '')),
                "pages_count": resp_json.get("pages_count", 0),
                "metadata": resp_json.get("metadata", {}),
            }
            with open(local_intermediate, "w", encoding="utf-8") as f:
                json.dump(safe_payload, f, ensure_ascii=False, indent=2)
            intermediate_file = local_intermediate
            logger.warning("‚ö†Ô∏è –ü—É—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–∑ —Å–µ—Ä–≤–∏—Å–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω Airflow. "
                           "–°–æ—Ö—Ä–∞–Ω–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞.")

        processing_time = resp_json.get("processing_time", time.time() - start_time)
        pages = resp_json.get("pages_count", 0)

        result = {
            "success": True,
            "document_info": {
                "title": filename.replace('.pdf', ''),
                "total_pages": pages,
                "processing_time": processing_time,
                "status": "success"
            },
            "intermediate_file": intermediate_file,
            "original_config": config,
            "processing_stats": {
                "pages_processed": pages,
                "ocr_used": options["use_ocr"],
                "processing_time_seconds": processing_time
            }
        }

        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='process_document_with_docling',
            processing_time=processing_time,
            pages_count=pages,
            success=True
        )
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ document-processor –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å")
        return result

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ document-processor: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='process_document_with_docling',
            processing_time=time.time() - start_time,
            success=False
        )
        return {
            "success": False,
            "error": error_msg,
            "original_config": config
        }

def process_document_fallback(input_file: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ Docling"""
    try:
        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –æ–±—Ä–∞–±–æ—Ç–∫—É (Docling –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        markdown_content = f"# {config['filename'].replace('.pdf', '')}\n\n"
        markdown_content += "–î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ fallback —Ä–µ–∂–∏–º–µ.\n\n"
        markdown_content += f"–§–∞–π–ª: {config['filename']}\n"
        markdown_content += f"–†–∞–∑–º–µ—Ä: {config['chinese_doc_analysis']['file_size_mb']:.2f} MB\n"
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        temp_dir = "/opt/airflow/temp"
        os.makedirs(temp_dir, exist_ok=True)
        intermediate_file = f"{temp_dir}/preprocessing_{config['timestamp']}.json"
        
        document_data = {
            'title': config['filename'].replace('.pdf', ''),
            'pages_count': config['chinese_doc_analysis']['estimated_pages'],
            'markdown_content': markdown_content,
            'raw_text': markdown_content,
            'metadata': {
                'fallback_mode': True,
                'processing_timestamp': config['timestamp']
            }
        }
        
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(document_data, f, ensure_ascii=False, indent=2)
        
        return {
            'success': True,
            'document_info': {
                'title': document_data['title'],
                'total_pages': document_data['pages_count'],
                'processing_time': 1.0,
                'status': 'fallback_success'
            },
            'intermediate_file': intermediate_file,
            'original_config': config,
            'processing_stats': {
                'fallback_mode': True,
                'processing_time_seconds': 1.0
            }
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': f"Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)}",
            'original_config': config
        }

def post_process_chinese_markdown(markdown: str) -> str:
    """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ Markdown –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    try:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        for chinese_term, english_term in CHINESE_DOC_CONFIG['tech_terms'].items():
            if chinese_term in markdown:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
                markdown = markdown.replace(chinese_term, f"{chinese_term} ({english_term})")
        
        # –£–ª—É—á—à–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        lines = markdown.split('\n')
        processed_lines = []
        
        for line in lines:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            for pattern in CHINESE_DOC_CONFIG['chinese_header_patterns']:
                import re
                if re.match(pattern, line.strip()):
                    if not line.strip().startswith('#'):
                        line = f"## {line.strip()}"
                    break
            
            processed_lines.append(line)
        
        # –£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        processed_markdown = '\n'.join(processed_lines)
        processed_markdown = improve_chinese_tables(processed_markdown)
        
        return processed_markdown
        
    except Exception as e:
        logger.warning(f"–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ markdown –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        return markdown

def improve_chinese_tables(markdown: str) -> str:
    """–£–ª—É—á—à–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü —Å –∫–∏—Ç–∞–π—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º"""
    try:
        import re
        
        # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∏—Ö —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        lines = markdown.split('\n')
        improved_lines = []
        in_table = False
        
        for line in lines:
            if '|' in line and len(line.split('|')) >= 3:
                if not in_table:
                    # –ù–∞—á–∞–ª–æ —Ç–∞–±–ª–∏—Ü—ã - –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    in_table = True
                    if not any(c in line for c in ['---', '===', '-+-']):
                        improved_lines.append(line)
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
                        cols = len([col for col in line.split('|') if col.strip()])
                        separator = '|' + '---|' * max(1, cols-2) + '|'
                        improved_lines.append(separator)
                        continue
                improved_lines.append(line)
            else:
                if in_table and line.strip() == '':
                    in_table = False
                improved_lines.append(line)
        
        return '\n'.join(improved_lines)
        
    except Exception as e:
        logger.warning(f"–£–ª—É—á—à–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}")
        return markdown

def count_chinese_characters(text: str) -> int:
    """–ü–æ–¥—Å—á–µ—Ç –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    try:
        return sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
    except:
        return 0

def prepare_for_next_stage(**context) -> Dict[str, Any]:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG"""
    start_time = time.time()
    
    try:
        result = context['task_instance'].xcom_pull(task_ids='process_document_with_docling')
        
        if not result.get('success'):
            raise AirflowException(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {result.get('error')}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        intermediate_file = result.get('intermediate_file')
        if not intermediate_file or not os.path.exists(intermediate_file):
            raise AirflowException("–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è DAG2
        next_stage_config = {
            'intermediate_file': intermediate_file,
            'original_config': result['original_config'],
            'dag1_metadata': {
                **result.get('processing_stats', {}),
                'completion_time': datetime.now().isoformat()
            },
            'dag1_completed': True,
            'ready_for_transformation': True,
            'chinese_document': True  # –§–ª–∞–≥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–∞–¥–∏–π
        }
        
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='prepare_for_next_stage',
            processing_time=time.time() - start_time,
            success=True
        )
        
        logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG")
        return next_stage_config
        
    except Exception as e:
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='prepare_for_next_stage',
            processing_time=time.time() - start_time,
            success=False
        )
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def notify_completion(**context) -> None:
    """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    try:
        result = context['task_instance'].xcom_pull(task_ids='process_document_with_docling')
        next_config = context['task_instance'].xcom_pull(task_ids='prepare_for_next_stage')
        
        if result and result.get('success'):
            stats = result.get('processing_stats', {})
            message = f"""
‚úÖ DOCUMENT PREPROCESSING –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û

üìÑ –§–∞–π–ª: {result['original_config']['filename']}
üìä –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('pages_processed', 'N/A')}
‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stats.get('processing_time_seconds', 0):.2f}—Å
üîç OCR –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: {'–î–∞' if stats.get('ocr_used') else '–ù–µ—Ç'}
üà∂ –ö–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤: {stats.get('chinese_chars_found', 0)}
üéØ –†–µ–∂–∏–º: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

üìÅ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª: {next_config.get('intermediate_file', 'N/A')}

‚úÖ –ì–æ—Ç–æ–≤ –∫ –ø–µ—Ä–µ–¥–∞—á–µ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç–∞–¥–∏—é
            """
            
            NotificationUtils.send_success_notification(context, result)
        else:
            error = result.get('error', 'Unknown error') if result else 'No result'
            message = f"""
‚ùå DOCUMENT PREPROCESSING –ó–ê–í–ï–†–®–ï–ù –° –û–®–ò–ë–ö–û–ô

üìÑ –§–∞–π–ª: {result['original_config']['filename'] if result else 'Unknown'}
‚ùå –û—à–∏–±–∫–∞: {error}
‚è∞ –í—Ä–µ–º—è: {datetime.now().isoformat()}

–¢—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
            """
            NotificationUtils.send_failure_notification(context, Exception(error))
        
        logger.info(message)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")

# ================================================================================
# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ó–ê–î–ê–ß
# ================================================================================

# –ó–∞–¥–∞—á–∞ 1: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
validate_input = PythonOperator(
    task_id='validate_input_file',
    python_callable=validate_input_file,
    execution_timeout=timedelta(minutes=5),
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 2: –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
process_document = PythonOperator(
    task_id='process_document_with_docling',
    python_callable=process_document_with_docling,
    execution_timeout=timedelta(hours=1),
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG
prepare_next = PythonOperator(
    task_id='prepare_for_next_stage',
    python_callable=prepare_for_next_stage,
    execution_timeout=timedelta(minutes=5),
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 4: –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
notify_task = PythonOperator(
    task_id='notify_completion',
    python_callable=notify_completion,
    trigger_rule='all_done',
    execution_timeout=timedelta(minutes=2),
    dag=dag
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
validate_input >> process_document >> prepare_next >> notify_task

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
def handle_processing_failure(context):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    try:
        failed_task = context['task_instance'].task_id
        exception = context.get('exception')
        
        error_message = f"""
üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –í DOCUMENT PREPROCESSING

–ó–∞–¥–∞—á–∞: {failed_task}
–û—à–∏–±–∫–∞: {str(exception) if exception else 'Unknown'}

–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:
1. –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π PDF —Ñ–∞–π–ª
2. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
3. –ü—Ä–æ–±–ª–µ–º—ã —Å Docling –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π
4. –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å PDF —Ñ–∞–π–ª–∞
- –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        """
        
        logger.error(error_message)
        NotificationUtils.send_failure_notification(context, exception)
        
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id=failed_task,
            processing_time=0,
            success=False
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ –æ—à–∏–±–æ–∫: {e}")

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫ –∫–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º
for task in dag.tasks:
    task.on_failure_callback = handle_processing_failure